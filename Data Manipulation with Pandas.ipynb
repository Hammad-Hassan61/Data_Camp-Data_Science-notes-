{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Data Manipulation with Pandas",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Pandas\nPandas is build on NumPy and Matplotlib\n- Numpy provide multidimensional arrays\n- Matplotlib provides tools for data visualization\n\nPandas is designed to work with **rectangular data** that is reprsented as **dataframes**. \n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exploring content of data set\n- head (only few rows)\n- info (name and data types of column and null values)\n- shape (return number of rows and column) method\n- describe (gives statistical summary of numerical columns)\n- values (gives values in 2d numPy array) method\n- columns (gives name of columns ) method\n- index (gives row names and count) method",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Sorting\n- .sort_values('column-name')\n- .sort_values('column-name', ascending = False)\n- .sort_values(['column-name','column-name']) **List of column names**\n- .sort_values(['column-name','column-name'], ascending = [False,True])",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Subsetting columns\nWe may want to zoom in on just one column. We can do this using the name of the DataFrame, followed by square brackets with a column name inside.\n\n- dogs[\"column-name\"]\n- dogs[[\"column-name\",\"column-name\"]] **List of column names**\n\n## Filters\nTo get only selected columns. For multiple columns use lists (symbol = []) inside the square brackets as use in sorting.\n- dogs[\"column-name\"] > 50 **Return true and false for every value**\n- dogs[ dogs[\"column-name\"] > 50] **Return actual rows**\n- You can use logical operators in conditions above as well\n\n- dogs[\"colour\"].isin ([\"Black\",\"Brown\"] ) **Called isin method, it is a condition, so must be used inside the another dogs[\"here\"]**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## New Column\nNew columns can be created by the existing columns. For example\n\ndogs['new-column'] = dogs['existing-column']/10 \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Summary statistics\n\n- dogs['column-name'].mean()\n- dogs['column-name'].median()\n- dogs['column-name'].mode()\n- dogs['column-name'].min()\n- dogs['column-name'].max()\n- dogs['column-name'].var()\n- dogs['column-name'].std()\n- dogs['column-name'].sum()\n- dogs['column-name'].quantile()\n\n\n## Aggregate (Agg method)\nThe aggregate, or agg, method allows you to compute custom summary statistics. Here, we create a function called pct30 that computes the thirtieth(30th) percentile of a DataFrame column. \n\n**def pct30(column)\n    return column.quantile(0.3)**\n    \nNow we can subset the weight column and call dot-agg, passing in the name of our function, pct30. It gives us the thirtieth percentile of the dogs' weights.\n\n**dogs['weight_kg'].agg(pct30)**\n\nYou can pass list of columns\n\n**dogs[['weight_kg','other_column']].agg(pct30)**\n\n\nAgg method can have more than one function.\n\ndef pct(column)\n    return column.quantile(0.40)\n\n**dogs['weight_kg'].agg([pct30,pct40])**\n\n## Comulitative frequency\n- dogs['column-name'].cumsum() (Commulitative frequency)\n- dogs['column-name'].cummax() \n- dogs['column-name'].cummin() \n- dogs['column-name'].cumprod() \n\nThese all return an entire column of a DataFrame, rather than a single number.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Counting\n  ## Droping duplicates\n**dogs.drop_duplicates(subset = \"column_name\" )**   \nSince Max and Max are different breeds, we can drop the rows with pairs of name and breed listed earlier in the dataset. To base our duplicate dropping on multiple columns, we can pass a list of column names to the subset argument, in this case, name and breed. Now both Maxes have been included, and we can start counting.\n**dogs.drop_duplicates(subset = [\"column_name\",\"column_name\"] )**\n\n- **dogs[\"column_name\"].value_counts()**\n- **dogs[\"column_name\"].value_counts(sort = True) (Display maximum to minimum count)**\n- **dogs[\"column_name\"].value_counts(normalize = True)**\nThe normalize argument can be used to turn the counts into proportions of the total. 25% of the dogs that go to this vet are Labradors.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Statistical summeries by group\nWhile computing summary statistics of entire columns may be useful, you can gain many insights from summaries of individual groups. For example, does one color of dog weigh more than another on average?\n\n**dogs[dogs['colour']=='brown']['weight_kg'].mean()**\n**dogs[dogs['colour']=='black']['weight_kg'].mean()**\nThat's where the groupby method comes in. \n\n**dogs.groupby(\"colour\")['weight_kg'].mean()**\n\nThis will give us the mean weight for each dog color. This was just one line of code compared to the five we had to write before to get the same results.\n\n**dogs.groupby(\"colour\")['weight_kg'].agg([min,max,sum])**\n\nJust like with ungrouped summary statistics, we can use the agg method to get multiple statistics. Here, we pass a list of functions into agg after grouping by color. This gives us the minimum, maximum, and sum of the different colored dogs' weights.\n\n**dogs.groupby([\"colour\",\"breed\"])['weight_kg'].agg([min,max,sum])**\n\nYou can also group by multiple columns and calculate summary statistics.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Pivot table\nn the last lesson, we grouped the dogs by color and calculated their mean weights. We can do the same thing using the pivot_table method. The \"values\" argument is the column that you want to summarize, and the index column is the column that you want to group by. By default, pivot_table takes the mean value for each group.\n\n**dogs.pivot_table(values= 'weight_kg', index= 'colour')**\n\nIf we want a different summary statistic, we can use the aggfunc argument and pass it a function. Here, we take the median for each dog color using NumPy's median function\n\n**dogs.pivot_table(values= 'weight_kg', index= 'colour', aggfunc = np.median )**\n\nTo get multiple summary statistics at a time, we can pass a list of functions to the aggfunc argument. Here, we get the mean and median for each dog color.\n\n**dogs.pivot_table(values= 'weight_kg', index= 'colour', aggfunc = [np.median,np.mode] )**\n\nYou also previously computed the mean weight grouped by two variables: color and breed. We can also do this using the pivot_table method. To group by two variables, we can pass a second variable name into the columns argument. While the result looks a little different than what we had before, it contains the same numbers. There are NaNs, or missing values, because there are no black Chihuahuas or gray Labradors in our dataset, for example.\n\n**dogs.pivot_table(values= 'weight_kg', index= 'colour', columns = 'breed',  aggfunc = [np.median,np.mode] )**\n\nInstead of having lots of missing values in our pivot table, we can have them filled in using the fill_value argument. Here, all of the NaNs get filled in with zeros.\n\n\n**dogs.pivot_table(values= 'weight_kg', index= 'colour', columns = 'breed', fill_value = 0, aggfunc = [np.median,np.mode] )**\n\nIf we set the margins argument to True, the last row and last column of the pivot table contain the mean of all the values in the column or row, not including the missing values that were filled in with Os. For example, in the last row of the Labrador column, we can see that the mean weight of the Labradors is 26 kilograms. In the last column of the Brown row, the mean weight of the Brown dogs is 24 kilograms. The value in the bottom right, in the last row and last column, is the mean weight of all the dogs in the dataset. Using margins equals True allows us to see a summary statistic for multiple levels of the dataset: the entire dataset, grouped by one variable, by another variable, and by two variables.\n\n**dogs.pivot_table(values= 'weight_kg', index= 'colour', columns = 'breed', fill_value = 0, margins = True, aggfunc = [np.median,np.mode] )**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Explicit indexes\n\nYou can move a column from the body of the DataFrame to the index. This is called \"setting an index,\" and it uses the set_index method. Notice that the output has changed slightly; in particular, a quick visual clue that name is now in the index is that the index values are left-aligned rather than right-aligned.\n\n**dogs.set_index('column name')**\n\nTo undo what you just did, you can reset the index - that is, you remove it. This is done via reset_index.\n\n**dogs.reset_index()**\n\nreset_index has a drop argument that allows you to discard an index. Here, setting drop to True entirely removes the dog names\n\n**dogs.reset_index(drop = True)**\n\nYou may be wondering why you should bother with indexes. The answer is that it makes subsetting code cleaner.\n\nThe values in the index don't need to be unique. Here, there are two Labradors in the index.\n\nYou can include multiple columns in the index by passing a list of column names to set_index\n\n**dogs.set_index(['name',column name'])**\n\nTo take a subset of rows at the outer level index, you pass a list of index values to loc.\n\n**dogs.loc[['Labarador','chihuahua']]**\n\nTo subset on inner levels, you need to pass a list of tuples. Here, the first tuple specifies Labrador at the outer level and Brown at the inner level. The resulting rows have to match all conditions from a tuple.\n\n**dogs.loc[[('Labarador','Brown'),('Labarador','Black') ]]**\n\nYou can also sort by index values using sort_index. By default, it sorts all index levels from outer to inner, in ascending order.\n\n**dogs.sort_index()**\n\nYou can control the sorting by passing lists to the level and ascending arguments.\n\n**dogs.sort_index(level = ['color','breed'], ascending = [True,False])**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Slicing and subsetting\n\n### Slicing rows\nTo slice the list, you pass first and last positions separated by a colon into square brackets.\n\n**breed[1:3] third element will not be included**\n\nYou can also slice DataFrames, but first, you need to sort the index.\n\nTo slice rows at the outer level of an index, you call loc, passing the first and last values separated by a colon. The full dataset is shown on the right for comparison. There are two differences compared to slicing lists. Rather than specifying row numbers, you specify index values. Secondly, notice that the final value is included. Here, last element (poodle) is included in the results.\n\n**dogs.loc[\"Chow Chow\": \"Poodle\"]**\n\nThe same technique doesn't work on inner index levels. Here, trying to slice from Tan to Grey returns an empty DataFrame instead of the six dogs we wanted. It's important to understand the danger here. pandas doesn't throw an error to let you know that there is a problem, so be careful when coding.\n\nThe correct approach to slicing at inner index levels is to pass the first and last positions as tuples. \n\n**dogs.loc[('Chow Chow','brown'):('Poodle','Grey')]**\n\n### Slicing columns\n\nSince DataFrames are two-dimensional objects, you can also slice columns. You do this by passing two arguments to loc. The simplest case involves subsetting columns but keeping all rows. To do this, pass a colon as the first argument to loc. As with slicing lists, a colon by itself means \"keep everything.\" The second argument takes column names as the first and last positions to slice on.\n\n**dogs.loc[:,'name':'height_cm']**\n\nYou can slice on rows and columns at the same time: simply pass the appropriate slice to each argument. Here, you see the previous two slices being performed in the same line of code.\n\n**dogs.loc[\n('Chow Chow','brown'):('Poodle','Grey')\n,\"name\":\"height_cm\"]**\n\n\nAn important use case of slicing is to subset DataFrames by a range of dates. To demonstrate this, let's set the date_of_birth column as the index and sort by this index.\n\n**dogs = dogs.set_index('date_of_birth').sort_index()**\n\nYou slice dates with the same syntax as other types. The first and last dates are passed as strings.\n\n**dogs.loc[\"2014-08-25\":\"2016-09-16\"]**\n\nOne helpful feature is that you can slice by partial dates. Here, the first and last positions are only specified as 2014 and 2016, with no month or day parts. pandas interprets this as slicing from the start of 2014 to the end of 2016; that is, all dates in 2014, 2015, and 2016.\n\n**dogs.loc[\"2014\":\"2016\"]**\n\nYou can also slice DataFrames by row or column number using the iloc method. This uses a similar syntax to slicing lists, except that there are two arguments: one for rows and one for columns. Notice that, like list slicing but unlike loc, the final values aren't included in the slice. In this case, the fifth row and fourth column aren't included.\n\n**dogs.iloc[2:5,1:4]**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Subsetting by pivot table\n\nPivot tables are just DataFrames with sorted indexes. That means that all the fun stuff you've learned so far this chapter can be used on them. In particular, the loc and slicing combination is ideal for subsetting pivot tables, like so\n\n**dogs_pivot.loc[\"Chow Chow\": \"Poodle\"]**\n\nhe methods for calculating summary statistics on a DataFrame, such as mean, have an axis argument. The default value is \"index,\" which means \"calculate the statistic across rows.\" Here, the mean is calculated for each color. That is, \"across the breeds.\" The behavior is the same as if you hadn't specified the axis argument.\n\n**dogs_pivot.mean(axis = \"index\")**\n\nTo calculate a summary statistic for each row, that is, \"across the columns,\" you set axis to \"columns.\" \n\n**dogs_pivot.mean(axis = \"columns\")**\n\nFor most DataFrames, setting the axis argument doesn't make any sense, since you'll have different data types in each column. Pivot tables are a special case since every column contains the same data type.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Visualizing your data\n\nimport matplotlib.pyplot as plt\n\n### Histogram\n\n**dog[\"column name\"].hist()**\n\n**dog[\"column name\"].hist(bins = 20)**\n\n### Bar plot\nBar plots can reveal relationships between a categorical variable and a numeric variable, like breed and weight. To compute the average weight of each breed, we group by breed, select the weight column, and take the mean, giving us the average weight of each breed.\n\n**avg_weight = dog.groupby(\"breed\")['weight_kg'].mean()**\n\n**avg_weight.plot(kind = bar)**\n\n### Line plot\n\nLine plots are great for visualizing changes in numeric variables over time. Lucky for us, a Labrador named Sully has been weighed by his owner every month - let's see how his weight has changed over the year. We can use the plot method again, but this time, we pass in three arguments: date as x, weight as y, and \"kind\" equals \"line.\" Sully's weight has fluctuated quite a bit over the year!\n\n**sully.plot(x=\"date\",y=\"weight_kg\",kind = \"line\")**\n\nWe may want to rotate the x-axis labels to make the text easier to read. This can be done by passing an angle in degrees with the \"rot\" argument. Here, we rotate the labels by 45 degrees.\n\n**sully.plot(x=\"date\",y=\"weight_kg\",kind = \"line\",rot=45)**\n\n### Scatter plots\n\nScatter plots are great for visualizing relationships between two numeric variables. To plot each dog's height versus their weight, we call the plot method with x equal to height_cm, y equal to weight_kg, and \"kind\" equal to \"scatter.\" From our plot, it looks like taller dogs tend to weigh more.\n\n**dog.plot(x=\"height\",y=\"weight\", kind= \"scatter\")**\n\n### Layering plots\n\nPlots can also be layered on top of one another. For example, we can create a histogram of female dogs' heights, and put a histogram of male dogs' heights on top, then call show. However, we can't tell which color represents which sex.\n\n**dogs[dogs['sex'] == \"M\"]['height'].hist()**\n**dogs[dogs['sex'] == \"F\"]['height'].hist()**\n\n#### Legend\nWe can use plt-dot-legend, passing in a list of labels, and then call show. Now we know which color is which, but we can't see what's going on behind the orange histogram.\n\n**plt.legend([\"F\",\"M\"])**\n\n#### Transperancy\nLet's fix this problem by making the histograms translucent. We can use hist's alpha argument, which takes a number. 0 means completely transparent that is, invisible, and 1 means completely opaque.\n\n**dogs[dogs['sex'] == \"M\"]['height'].hist(alpha = 0.7)**\n**dogs[dogs['sex'] == \"F\"]['height'].hist(aplha = 0.7)**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Missing values\n\nYou could be given a DataFrame that has missing values, so it's important to know how to handle them.\nMost data is not perfect - there's always a possibility that there are some pieces missing from your dataset. For example, maybe on the day that Bella and Cooper's owner weighed them,\nthe scale was broken. Now we have two missing values in our dataset.\n\n\nIn a pandas DataFrame, missing values are indicated with N-a-N, which stands for \"not a number.\"\n\nWhen you first get a DataFrame, it's a good idea to get a sense of whether it contains any missing values, and if so, how many. That's where the isna method comes in. When we call isna on a DataFrame, we get a Boolean for every single value indicating whether the value is missing or not, but this isn't very helpful when you're working with a lot of data.\n\n**dogs.isna()**\n\nIf we chain dot-isna with dot-any, we get one value for each variable that tells us if there are any missing values in that column. Here, we see that there's at least one missing value in the weight column, but not in any of the others.\n\n**dogs.isna().any()**\n\nSince taking the sum of Booleans is the same thing as counting the number of Trues, we can combine sum with isna to count the number of NaNs in each column.\n\n**dogs.isna().sum()**\n\nWe can use those counts to visualize the missing values in the dataset using a bar plot. Plots like this are more interesting when you have missing data across different variables, while here, only weights are missing. Now that we know there are missing values in the dataset, what can we do about them?\n\nOne option is to remove the rows in the DataFrame that contain missing values. This can be done using the dropna method. However, this may not be ideal if you have a lot of missing data, since that means losing a lot of observations.\n\n**dogs.dropna()**\n\nAnother option is to replace missing values with another value. The fillna method takes in a value, and all NaNs will be replaced with this value. There are also many sophisticated techniques for replacing missing values, which you can learn more about in our course about missing data.\n\n**dogs.fillna(0)**\n\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Reading and writing to csv\n\n**pd.read_csv(\"\")**\n\n**DataFramce.to_csv(\"name\")**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}